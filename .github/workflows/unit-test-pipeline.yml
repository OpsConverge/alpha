name: Unit Test Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      framework:
        description: 'Test framework to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - junit
        - pytest
        - jest
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'normal'
        type: choice
        options:
        - normal
        - force_pass
        - force_fail
        - mixed_results
      include_failing_tests:
        description: 'Include intentionally failing tests'
        required: false
        default: 'true'
        type: choice
        options:
        - true
        - false

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  JAVA_VERSION: '17'
  TEST_MODE: ${{ github.event.inputs.test_mode || 'normal' }}
  INCLUDE_FAILING_TESTS: ${{ github.event.inputs.include_failing_tests || 'true' }}

jobs:
  # JUnit Unit Tests (Java)
  junit-unit-tests:
    name: JUnit Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'junit' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up JDK ${{ env.JAVA_VERSION }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
        cache: 'maven'
        
    - name: Cache Maven packages
      uses: actions/cache@v3
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
        
    - name: Configure test mode for Java
      run: |
        cd java-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/StringServiceTest.java
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/assertEquals(8.0, result/assertEquals(999.0, result/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/assertEquals("olleh", result/assertEquals("FAIL", result/' src/test/java/com/example/unit/StringServiceTest.java
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/@Test.*failing.*calculator/# @Test.*failing.*calculator/' src/test/java/com/example/unit/CalculatorServiceTest.java
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/StringServiceTest.java
        fi
        
    - name: Run JUnit unit tests
      run: |
        cd java-service
        mvn clean test -Dtest=**/unit/** -DfailIfNoTests=false
        
    - name: Generate JUnit test report
      if: always()
      run: |
        cd java-service
        mvn surefire-report:report-only
        
    - name: Upload JUnit test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: junit-test-results
        path: java-service/target/surefire-reports/
        retention-days: 30
        
    - name: Upload JUnit coverage report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: junit-coverage-report
        path: java-service/target/site/jacoco/
        retention-days: 30

  # pytest Unit Tests (Python)
  pytest-unit-tests:
    name: pytest Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'pytest' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r python-service/requirements.txt
        pip install pytest pytest-cov pytest-html pytest-xdist
        
    - name: Configure test mode for Python
      run: |
        cd python-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_calculator_service.py
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_string_service.py
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/assert result == 8.0/assert result == 999.0/' tests/unit/test_calculator_service.py
          sed -i 's/assert result == "olleh"/assert result == "FAIL"/' tests/unit/test_string_service.py
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/def test_add_positive_numbers_failing/# def test_add_positive_numbers_failing/' tests/unit/test_calculator_service.py
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_calculator_service.py
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_string_service.py
        fi
        
    - name: Run pytest unit tests
      run: |
        cd python-service
        pytest tests/unit/ -v --cov=src --cov-report=html --cov-report=xml --cov-report=term-missing --junitxml=pytest-results.xml
        
    - name: Upload pytest test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pytest-test-results
        path: python-service/pytest-results.xml
        retention-days: 30
        
    - name: Upload pytest coverage report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pytest-coverage-report
        path: python-service/htmlcov/
        retention-days: 30

  # Jest Unit Tests (JavaScript/Node.js)
  jest-unit-tests:
    name: Jest Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'jest' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: node-service/package-lock.json
        
    - name: Install dependencies
      run: |
        cd node-service
        npm ci
        
    - name: Configure test mode for Node.js
      run: |
        cd node-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/calculatorService.test.js
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/stringService.test.js
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/expect(result).toBe(8)/expect(result).toBe(999)/' tests/unit/calculatorService.test.js
          sed -i 's/expect(result).toBe("olleh")/expect(result).toBe("FAIL")/' tests/unit/stringService.test.js
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/test.*INTENTIONALLY FAILING.*calculator/# test.*INTENTIONALLY FAILING.*calculator/' tests/unit/calculatorService.test.js
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/calculatorService.test.js
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/stringService.test.js
        fi
      
    - name: Run Jest unit tests
      run: |
        cd node-service
        npm run test:unit
        
    - name: Upload Jest test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: jest-test-results
        path: node-service/coverage/
        retention-days: 30

  # Combined Unit Test Report
  unit-test-summary:
    name: Unit Test Summary
    runs-on: ubuntu-latest
    needs: [junit-unit-tests, pytest-unit-tests, jest-unit-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-results/
        
    - name: Generate test summary
      run: |
        echo "## Unit Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🎛️ Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Mode:** ${{ env.TEST_MODE }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Include Failing Tests:** ${{ env.INCLUDE_FAILING_TESTS }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Framework:** ${{ github.event.inputs.framework || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # JUnit Results
        if [ -d "test-results/junit-test-results" ]; then
          echo "### JUnit Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### JUnit Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        # pytest Results
        if [ -d "test-results/pytest-test-results" ]; then
          echo "### pytest Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### pytest Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Jest Results
        if [ -d "test-results/jest-test-results" ]; then
          echo "### Jest Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### Jest Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 **Coverage Reports:**" >> $GITHUB_STEP_SUMMARY
        echo "- JUnit: Available in junit-coverage-report artifact" >> $GITHUB_STEP_SUMMARY
        echo "- pytest: Available in pytest-coverage-report artifact" >> $GITHUB_STEP_SUMMARY
        echo "- Jest: Available in jest-test-results artifact" >> $GITHUB_STEP_SUMMARY

  # Quality Gates
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [junit-unit-tests, pytest-unit-tests, jest-unit-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-results/
        
    - name: Quality Gate - Test Execution
      run: |
        echo "## Quality Gate Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check if all test jobs completed successfully
        if [[ "${{ needs.junit-unit-tests.result }}" == "success" && "${{ needs.pytest-unit-tests.result }}" == "success" && "${{ needs.jest-unit-tests.result }}" == "success" ]]; then
          echo "✅ **All unit tests passed**" >> $GITHUB_STEP_SUMMARY
          echo "🎯 **Quality Gate: PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Some unit tests failed**" >> $GITHUB_STEP_SUMMARY
          echo "🚫 **Quality Gate: FAILED**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Failed jobs:" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.junit-unit-tests.result }}" != "success" ]] && echo "- JUnit Tests: ${{ needs.junit-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.pytest-unit-tests.result }}" != "success" ]] && echo "- pytest Tests: ${{ needs.pytest-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.jest-unit-tests.result }}" != "success" ]] && echo "- Jest Tests: ${{ needs.jest-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Exit with failure if any tests failed
        if [[ "${{ needs.junit-unit-tests.result }}" != "success" || "${{ needs.pytest-unit-tests.result }}" != "success" || "${{ needs.jest-unit-tests.result }}" != "success" ]]; then
          exit 1
        fi
