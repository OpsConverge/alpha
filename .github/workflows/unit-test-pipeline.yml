name: Unit Test Pipeline

on:
  # push:
  #   branches: [ main, develop ]
  # pull_request:
  #   branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      framework:
        description: 'Test framework to run'
        required: false
        default: 'all'
        type: choice
        options:
        - all
        - junit
        - pytest
        - jest
      test_mode:
        description: 'Test execution mode'
        required: false
        default: 'normal'
        type: choice
        options:
        - normal
        - force_pass
        - force_fail
        - mixed_results
      include_failing_tests:
        description: 'Include intentionally failing tests'
        required: false
        default: 'true'
        type: choice
        options:
        - true
        - false

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  JAVA_VERSION: '17'
  TEST_MODE: ${{ github.event.inputs.test_mode || 'normal' }}
  INCLUDE_FAILING_TESTS: ${{ github.event.inputs.include_failing_tests || 'true' }}

jobs:
  # JUnit Unit Tests (Java)
  junit-unit-tests:
    name: JUnit Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'junit' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up JDK ${{ env.JAVA_VERSION }}
      uses: actions/setup-java@v4
      with:
        java-version: ${{ env.JAVA_VERSION }}
        distribution: 'temurin'
        cache: 'maven'
        
    - name: Cache Maven packages
      uses: actions/cache@v3
      with:
        path: ~/.m2
        key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
        restore-keys: ${{ runner.os }}-m2
        
    - name: Configure test mode for Java
      run: |
        cd java-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/StringServiceTest.java
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/assertEquals(8.0, result/assertEquals(999.0, result/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/assertEquals("olleh", result/assertEquals("FAIL", result/' src/test/java/com/example/unit/StringServiceTest.java
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/@Test.*failing.*calculator/# @Test.*failing.*calculator/' src/test/java/com/example/unit/CalculatorServiceTest.java
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/CalculatorServiceTest.java
          sed -i 's/@Test.*failing/# @Test.*failing/' src/test/java/com/example/unit/StringServiceTest.java
        fi
        
    - name: Run JUnit unit tests
      run: |
        cd java-service
        mvn clean test -Dtest=**/unit/** -DfailIfNoTests=false
        
    - name: Generate JUnit test report
      if: always()
      run: |
        cd java-service
        mvn surefire-report:report-only
        
    - name: Generate industry standard JUnit XML
      if: always()
      run: |
        cd java-service
        # Maven Surefire already generates JUnit XML format in target/surefire-reports/
        # This is the industry standard format that most CI/CD platforms can parse
        echo "JUnit XML reports generated in target/surefire-reports/"
        ls -la target/surefire-reports/
        
    - name: Generate standardized test results for platform
      if: always()
      run: |
        cd java-service
        echo "Generating standardized test results for platform..."
        
        # Parse surefire reports to extract test statistics
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        SKIPPED_TESTS=0
        
        # Count tests from surefire reports
        if [ -d "target/surefire-reports" ]; then
          for report in target/surefire-reports/*.txt; do
            if [ -f "$report" ]; then
              # Extract test counts from surefire report
              TESTS_RUN=$(grep "Tests run:" "$report" | head -1 | sed 's/.*Tests run: \([0-9]*\).*/\1/')
              FAILURES=$(grep "Failures:" "$report" | head -1 | sed 's/.*Failures: \([0-9]*\).*/\1/')
              ERRORS=$(grep "Errors:" "$report" | head -1 | sed 's/.*Errors: \([0-9]*\).*/\1/')
              SKIPPED=$(grep "Skipped:" "$report" | head -1 | sed 's/.*Skipped: \([0-9]*\).*/\1/')
              
              TOTAL_TESTS=$((TOTAL_TESTS + TESTS_RUN))
              FAILED_TESTS=$((FAILED_TESTS + FAILURES + ERRORS))
              SKIPPED_TESTS=$((SKIPPED_TESTS + SKIPPED))
            fi
          done
        fi
        
        PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS - SKIPPED_TESTS))
        
        # Create standardized test results file
        cat > test-results-summary.json << EOF
        {
          "framework": "junit",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "test_mode": "${{ env.TEST_MODE }}",
          "include_failing_tests": "${{ env.INCLUDE_FAILING_TESTS }}",
          "summary": {
            "total_tests": $TOTAL_TESTS,
            "passed_tests": $PASSED_TESTS,
            "failed_tests": $FAILED_TESTS,
            "skipped_tests": $SKIPPED_TESTS,
            "success_rate": $([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0")
          },
          "status": "$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")"
        }
        EOF
        
        # Also create a simple CSV format for easy parsing
        echo "framework,test_mode,include_failing_tests,total_tests,passed_tests,failed_tests,skipped_tests,success_rate,status" > test-results.csv
        echo "junit,${{ env.TEST_MODE }},${{ env.INCLUDE_FAILING_TESTS }},$TOTAL_TESTS,$PASSED_TESTS,$FAILED_TESTS,$SKIPPED_TESTS,$([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0"),$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")" >> test-results.csv
        
        echo "Generated test results:"
        cat test-results-summary.json
        echo ""
        echo "CSV format:"
        cat test-results.csv
        
    - name: Upload JUnit test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: junit-test-results
        path: |
          java-service/target/surefire-reports/
          java-service/test-results-summary.json
          java-service/test-results.csv
        retention-days: 30
        
    - name: Upload JUnit XML (Industry Standard)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: junit-xml-standard
        path: java-service/target/surefire-reports/*.xml
        retention-days: 30
        
    - name: Upload JUnit coverage report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: junit-coverage-report
        path: java-service/target/site/jacoco/
        retention-days: 30

  # pytest Unit Tests (Python)
  pytest-unit-tests:
    name: pytest Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'pytest' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python ${{ env.PYTHON_VERSION }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
          
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r python-service/requirements.txt
        pip install pytest pytest-cov pytest-html pytest-xdist
        
    - name: Configure test mode for Python
      run: |
        cd python-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_calculator_service.py
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_string_service.py
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/assert result == 8.0/assert result == 999.0/' tests/unit/test_calculator_service.py
          sed -i 's/assert result == "olleh"/assert result == "FAIL"/' tests/unit/test_string_service.py
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/def test_add_positive_numbers_failing/# def test_add_positive_numbers_failing/' tests/unit/test_calculator_service.py
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_calculator_service.py
          sed -i 's/def test_.*_failing/# def test_.*_failing/' tests/unit/test_string_service.py
        fi
        
    - name: Run pytest unit tests
      run: |
        cd python-service
        # Generate industry standard JUnit XML format
        # Use -p no:warnings to suppress warnings and --import-mode=importlib for better import handling
        pytest tests/unit/ -v --cov=src --cov-report=html --cov-report=xml --cov-report=term-missing --junitxml=junit.xml --import-mode=importlib
        
    - name: Generate standardized test results for platform
      if: always()
      run: |
        cd python-service
        echo "Generating standardized test results for platform..."
        
        # Parse pytest XML results to extract test statistics
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        SKIPPED_TESTS=0
        
        # Extract test counts from pytest XML report
        if [ -f "pytest-results.xml" ]; then
          TOTAL_TESTS=$(grep -o 'testsuite.*tests="[0-9]*"' pytest-results.xml | head -1 | sed 's/.*tests="\([0-9]*\)".*/\1/')
          FAILED_TESTS=$(grep -o 'testsuite.*failures="[0-9]*"' pytest-results.xml | head -1 | sed 's/.*failures="\([0-9]*\)".*/\1/')
          SKIPPED_TESTS=$(grep -o 'testsuite.*skipped="[0-9]*"' pytest-results.xml | head -1 | sed 's/.*skipped="\([0-9]*\)".*/\1/')
          ERRORS=$(grep -o 'testsuite.*errors="[0-9]*"' pytest-results.xml | head -1 | sed 's/.*errors="\([0-9]*\)".*/\1/')
          
          # Handle cases where attributes might not exist
          TOTAL_TESTS=${TOTAL_TESTS:-0}
          FAILED_TESTS=${FAILED_TESTS:-0}
          SKIPPED_TESTS=${SKIPPED_TESTS:-0}
          ERRORS=${ERRORS:-0}
          
          FAILED_TESTS=$((FAILED_TESTS + ERRORS))
        fi
        
        PASSED_TESTS=$((TOTAL_TESTS - FAILED_TESTS - SKIPPED_TESTS))
        
        # Create standardized test results file
        cat > test-results-summary.json << EOF
        {
          "framework": "pytest",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "test_mode": "${{ env.TEST_MODE }}",
          "include_failing_tests": "${{ env.INCLUDE_FAILING_TESTS }}",
          "summary": {
            "total_tests": $TOTAL_TESTS,
            "passed_tests": $PASSED_TESTS,
            "failed_tests": $FAILED_TESTS,
            "skipped_tests": $SKIPPED_TESTS,
            "success_rate": $([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0")
          },
          "status": "$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")"
        }
        EOF
        
        # Also create a simple CSV format for easy parsing
        echo "framework,test_mode,include_failing_tests,total_tests,passed_tests,failed_tests,skipped_tests,success_rate,status" > test-results.csv
        echo "pytest,${{ env.TEST_MODE }},${{ env.INCLUDE_FAILING_TESTS }},$TOTAL_TESTS,$PASSED_TESTS,$FAILED_TESTS,$SKIPPED_TESTS,$([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0"),$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")" >> test-results.csv
        
        echo "Generated test results:"
        cat test-results-summary.json
        echo ""
        echo "CSV format:"
        cat test-results.csv
        
    - name: Upload pytest test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pytest-test-results
        path: |
          python-service/junit.xml
          python-service/test-results-summary.json
          python-service/test-results.csv
        retention-days: 30
        
    - name: Upload pytest JUnit XML (Industry Standard)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pytest-junit-xml-standard
        path: python-service/junit.xml
        retention-days: 30
        

        
    - name: Upload pytest coverage report
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: pytest-coverage-report
        path: python-service/htmlcov/
        retention-days: 30

  # Jest Unit Tests (JavaScript/Node.js)
  jest-unit-tests:
    name: Jest Unit Tests
    runs-on: ubuntu-latest
    if: github.event.inputs.framework == 'jest' || github.event.inputs.framework == 'all' || github.event.inputs.framework == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Node.js ${{ env.NODE_VERSION }}
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: node-service/package-lock.json
        
    - name: Install dependencies
      run: |
        cd node-service
        npm ci
        
    - name: Configure test mode for Node.js
      run: |
        cd node-service
        echo "Configuring test mode: ${{ env.TEST_MODE }}"
        echo "Include failing tests: ${{ env.INCLUDE_FAILING_TESTS }}"
        
        if [ "${{ env.TEST_MODE }}" = "force_pass" ]; then
          echo "🟢 Force Pass Mode: Commenting out failing tests"
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/calculatorService.test.js
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/stringService.test.js
        elif [ "${{ env.TEST_MODE }}" = "force_fail" ]; then
          echo "🔴 Force Fail Mode: Making all tests fail"
          sed -i 's/expect(result).toBe(8)/expect(result).toBe(999)/' tests/unit/calculatorService.test.js
          sed -i 's/expect(result).toBe("olleh")/expect(result).toBe("FAIL")/' tests/unit/stringService.test.js
        elif [ "${{ env.TEST_MODE }}" = "mixed_results" ]; then
          echo "🟡 Mixed Results Mode: Some tests will pass, some will fail"
          # Keep some failing tests, fix others
          sed -i 's/test.*INTENTIONALLY FAILING.*calculator/# test.*INTENTIONALLY FAILING.*calculator/' tests/unit/calculatorService.test.js
        fi
        
        if [ "${{ env.INCLUDE_FAILING_TESTS }}" = "false" ]; then
          echo "🚫 Excluding intentionally failing tests"
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/calculatorService.test.js
          sed -i 's/test.*INTENTIONALLY FAILING/# test.*INTENTIONALLY FAILING/' tests/unit/stringService.test.js
        fi
      
    - name: Run Jest unit tests
      run: |
        cd node-service
        # Generate industry standard JUnit XML format using jest-junit
        npm run test:unit -- --testResultsProcessor=jest-junit --coverage --coverageReporters=json --coverageReporters=text
        
        # Verify JUnit XML was generated
        if [ -f "junit.xml" ]; then
          echo "✅ JUnit XML generated successfully"
          ls -la junit.xml
        else
          echo "❌ JUnit XML not found, checking for alternative locations"
          find . -name "*.xml" -type f
        fi
        
    - name: Generate standardized test results for platform
      if: always()
      run: |
        cd node-service
        echo "Generating standardized test results for platform..."
        
        # Parse Jest JSON results to extract test statistics
        TOTAL_TESTS=0
        PASSED_TESTS=0
        FAILED_TESTS=0
        SKIPPED_TESTS=0
        
        # Extract test counts from Jest JSON report
        if [ -f "jest-results.json" ]; then
          TOTAL_TESTS=$(jq -r '.numTotalTests' jest-results.json 2>/dev/null || echo "0")
          PASSED_TESTS=$(jq -r '.numPassedTests' jest-results.json 2>/dev/null || echo "0")
          FAILED_TESTS=$(jq -r '.numFailedTests' jest-results.json 2>/dev/null || echo "0")
          SKIPPED_TESTS=$(jq -r '.numPendingTests' jest-results.json 2>/dev/null || echo "0")
          
          # Handle cases where jq might not be available or values are null
          TOTAL_TESTS=${TOTAL_TESTS:-0}
          PASSED_TESTS=${PASSED_TESTS:-0}
          FAILED_TESTS=${FAILED_TESTS:-0}
          SKIPPED_TESTS=${SKIPPED_TESTS:-0}
        fi
        
        # Create standardized test results file
        cat > test-results-summary.json << EOF
        {
          "framework": "jest",
          "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
          "test_mode": "${{ env.TEST_MODE }}",
          "include_failing_tests": "${{ env.INCLUDE_FAILING_TESTS }}",
          "summary": {
            "total_tests": $TOTAL_TESTS,
            "passed_tests": $PASSED_TESTS,
            "failed_tests": $FAILED_TESTS,
            "skipped_tests": $SKIPPED_TESTS,
            "success_rate": $([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0")
          },
          "status": "$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")"
        }
        EOF
        
        # Also create a simple CSV format for easy parsing
        echo "framework,test_mode,include_failing_tests,total_tests,passed_tests,failed_tests,skipped_tests,success_rate,status" > test-results.csv
        echo "jest,${{ env.TEST_MODE }},${{ env.INCLUDE_FAILING_TESTS }},$TOTAL_TESTS,$PASSED_TESTS,$FAILED_TESTS,$SKIPPED_TESTS,$([ $TOTAL_TESTS -gt 0 ] && echo "scale=2; $PASSED_TESTS * 100 / $TOTAL_TESTS" | bc || echo "0"),$([ $FAILED_TESTS -eq 0 ] && echo "PASSED" || echo "FAILED")" >> test-results.csv
        
        echo "Generated test results:"
        cat test-results-summary.json
        echo ""
        echo "CSV format:"
        cat test-results.csv
        
    - name: Upload Jest test results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: jest-test-results
        path: |
          node-service/coverage/
          node-service/test-results-summary.json
          node-service/test-results.csv
        retention-days: 30
        
    - name: Upload Jest JUnit XML (Industry Standard)
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: jest-junit-xml-standard
        path: node-service/junit.xml
        retention-days: 30
        


  # Combined Unit Test Report
  unit-test-summary:
    name: Unit Test Summary
    runs-on: ubuntu-latest
    needs: [junit-unit-tests, pytest-unit-tests, jest-unit-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download all test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-results/
        
    - name: Generate test summary
      run: |
        echo "## Unit Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🎛️ Test Configuration" >> $GITHUB_STEP_SUMMARY
        echo "- **Test Mode:** ${{ env.TEST_MODE }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Include Failing Tests:** ${{ env.INCLUDE_FAILING_TESTS }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Framework:** ${{ github.event.inputs.framework || 'all' }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # JUnit Results
        if [ -d "test-results/junit-test-results" ]; then
          echo "### JUnit Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### JUnit Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        # pytest Results
        if [ -d "test-results/pytest-test-results" ]; then
          echo "### pytest Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### pytest Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Jest Results
        if [ -d "test-results/jest-test-results" ]; then
          echo "### Jest Tests ✅" >> $GITHUB_STEP_SUMMARY
          echo "- Test results available in artifacts" >> $GITHUB_STEP_SUMMARY
        else
          echo "### Jest Tests ❌" >> $GITHUB_STEP_SUMMARY
          echo "- No test results found" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 **Coverage Reports:**" >> $GITHUB_STEP_SUMMARY
        echo "- JUnit: Available in junit-coverage-report artifact" >> $GITHUB_STEP_SUMMARY
        echo "- pytest: Available in pytest-coverage-report artifact" >> $GITHUB_STEP_SUMMARY
        echo "- Jest: Available in jest-test-results artifact" >> $GITHUB_STEP_SUMMARY

  # Quality Gates
  quality-gates:
    name: Quality Gates
    runs-on: ubuntu-latest
    needs: [junit-unit-tests, pytest-unit-tests, jest-unit-tests]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Download test artifacts
      uses: actions/download-artifact@v4
      with:
        path: test-results/
        
    - name: Quality Gate - Test Execution
      run: |
        echo "## Quality Gate Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        # Check if all test jobs completed successfully
        if [[ "${{ needs.junit-unit-tests.result }}" == "success" && "${{ needs.pytest-unit-tests.result }}" == "success" && "${{ needs.jest-unit-tests.result }}" == "success" ]]; then
          echo "✅ **All unit tests passed**" >> $GITHUB_STEP_SUMMARY
          echo "🎯 **Quality Gate: PASSED**" >> $GITHUB_STEP_SUMMARY
        else
          echo "❌ **Some unit tests failed**" >> $GITHUB_STEP_SUMMARY
          echo "🚫 **Quality Gate: FAILED**" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Failed jobs:" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.junit-unit-tests.result }}" != "success" ]] && echo "- JUnit Tests: ${{ needs.junit-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.pytest-unit-tests.result }}" != "success" ]] && echo "- pytest Tests: ${{ needs.pytest-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
          [[ "${{ needs.jest-unit-tests.result }}" != "success" ]] && echo "- Jest Tests: ${{ needs.jest-unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        fi
        
        # Exit with failure if any tests failed
        if [[ "${{ needs.junit-unit-tests.result }}" != "success" || "${{ needs.pytest-unit-tests.result }}" != "success" || "${{ needs.jest-unit-tests.result }}" != "success" ]]; then
          exit 1
        fi
